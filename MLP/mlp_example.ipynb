{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron (MLP) Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob, os\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "try:\n",
    "    for kgpu in range(len(physical_devices)):\n",
    "        tf.config.experimental.set_memory_growth(physical_devices[kgpu], True)\n",
    "except:\n",
    "    # Invalid device or cannot modify virtual devices once initialized.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build data pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input and output variable list\n",
    "- Note that ptend_t and ptend_q0001 are not in the output (mlo) netcdf files, but calculated real-time on a tf Dataset object.\n",
    "- Variable list: https://docs.google.com/spreadsheets/d/1ljRfHq6QB36u0TuoxQXcV4_DSQUR0X4UimZ4QHR8f9M/edit#gid=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in/out variable lists\n",
    "vars_mli = ['state_t','state_q0001','state_ps','pbuf_SOLIN', 'pbuf_LHFLX', 'pbuf_SHFLX']\n",
    "vars_mlo = ['ptend_t','ptend_q0001','cam_out_NETSW','cam_out_FLWDS','cam_out_PRECSC','cam_out_PRECC','cam_out_SOLS','cam_out_SOLL','cam_out_SOLSD','cam_out_SOLLD']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf Dataset pipeline\n",
    "- ref: https://www.noahbrenowitz.com/post/loading_netcdfs/\n",
    "- ref: https://www.tensorflow.org/api_docs/python/tf/data/Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "44519b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_path = \"/home/alvarovh/code/cse598_climate_proj/ClimSim/preprocessing/normalizations/\"\n",
    "root_train_path = \"/nfs/turbo/coe-mihalcea/alvarovh/climsim/climsim_lowres_twomonths/datasets--LEAP--ClimSim_low-res/snapshots/bab82a2ebdc750a0134ddcd0d5813867b92eed2a/train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "50dd1cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "mli_mean = xr.open_dataset(norm_path + 'inputs/input_mean.nc')\n",
    "mli_max = xr.open_dataset(norm_path + 'inputs/input_max.nc')\n",
    "mli_min = xr.open_dataset(norm_path + 'inputs/input_min.nc')\n",
    "mlo_scale = xr.open_dataset(norm_path + 'outputs/output_scale.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_nc_dir_with_generator(filelist:list):\n",
    "    def gen():\n",
    "        for file in filelist:\n",
    "            \n",
    "            # read mli\n",
    "            ds = xr.open_dataset(file, engine='netcdf4')\n",
    "            ds = ds[vars_mli]\n",
    "            \n",
    "            # read mlo\n",
    "            dso = xr.open_dataset(file.replace('.mli.','.mlo.'), engine='netcdf4')\n",
    "            \n",
    "            # make mlo variales: ptend_t and ptend_q0001\n",
    "            dso['ptend_t'] = (dso['state_t'] - ds['state_t'])/1200 # T tendency [K/s]\n",
    "            dso['ptend_q0001'] = (dso['state_q0001'] - ds['state_q0001'])/1200 # Q tendency [kg/kg/s]\n",
    "            dso = dso[vars_mlo]\n",
    "            \n",
    "            # normalizatoin, scaling\n",
    "            ds = (ds-mli_mean)/(mli_max-mli_min)\n",
    "            dso = dso*mlo_scale\n",
    "\n",
    "            # stack\n",
    "            #ds = ds.stack({'batch':{'sample','ncol'}})\n",
    "            ds = ds.stack({'batch':{'ncol'}})\n",
    "            ds = ds.to_stacked_array(\"mlvar\", sample_dims=[\"batch\"], name='mli')\n",
    "            #dso = dso.stack({'batch':{'sample','ncol'}})\n",
    "            dso = dso.stack({'batch':{'ncol'}})\n",
    "            dso = dso.to_stacked_array(\"mlvar\", sample_dims=[\"batch\"], name='mlo')\n",
    "            \n",
    "            yield (ds.values, dso.values)\n",
    "\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        output_types=(tf.float64, tf.float64),\n",
    "        output_shapes=((None,124),(None,128))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate tf.data.Dataset object here\n",
    "- Dataset file size and dimensions: https://docs.google.com/document/d/1HgfZZJM0SygjWvSAJ5kSfql9aXUFkvLybL36p-vmdZc/edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "85ea836b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Total # of input files after applying limit: 200\n",
      "[VAL] Total # of input files for validation (10% of training): 20\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import glob\n",
    "\n",
    "# Parameters for debugging\n",
    "train_file_limit = 200  # Number of files to use for training\n",
    "val_proportion = 0.1    # Percentage of training data used for validation\n",
    "\n",
    "# Training files from year 0001, month 02\n",
    "f_mli = glob.glob(root_train_path + '/0002-01/E3SM-MMF.mli.0002-01-*.nc')\n",
    "random.shuffle(f_mli)\n",
    "\n",
    "# Apply the limit to the training files\n",
    "f_mli = f_mli[:train_file_limit]\n",
    "print(f'[TRAIN] Total # of input files after applying limit: {len(f_mli)}')\n",
    "\n",
    "# Load and prepare the training dataset\n",
    "tds = load_nc_dir_with_generator(f_mli)\n",
    "tds = tds.shuffle(buffer_size=shuffle_buffer, reshuffle_each_iteration=True)\n",
    "tds = tds.prefetch(buffer_size=4)\n",
    "\n",
    "# Validation files from year 0002, month 02\n",
    "# Using 10% of the training files as validation\n",
    "f_mli_val = f_mli[:int(train_file_limit * val_proportion)]\n",
    "print(f'[VAL] Total # of input files for validation (10% of training): {len(f_mli_val)}')\n",
    "\n",
    "# Load and prepare the validation dataset\n",
    "tds_val = load_nc_dir_with_generator(f_mli_val)\n",
    "tds_val = tds_val.shuffle(buffer_size=shuffle_buffer, reshuffle_each_iteration=True)\n",
    "tds_val = tds_val.prefetch(buffer_size=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle_buffer=384*12\n",
    "\n",
    "# # for training\n",
    "\n",
    "# # # First 5 days of each month for the first 6 years\n",
    "# # f_mli1 = glob.glob('/pscratch/sd/s/sungduk/hugging/E3SM-MMF_ne4/train/*/E3SM-MMF.mli.000[123456]-*-0[12345]-*.nc')\n",
    "# # f_mli2 = glob.glob('/pscratch/sd/s/sungduk/hugging/E3SM-MMF_ne4/train/*/E3SM-MMF.mli.0007-01-0[12345]-*.nc')\n",
    "# # f_mli = [*f_mli1, *f_mli2]\n",
    "\n",
    "# # every 10th sample\n",
    "# f_mli1 = glob.glob(root_train_path + '/*/E3SM-MMF.mli.000[123456]-*-*-*.nc')\n",
    "# f_mli2 = glob.glob(root_train_path + '/*/E3SM-MMF.mli.0007-01-*-*.nc')\n",
    "# f_mli = sorted([*f_mli1, *f_mli2])\n",
    "# random.shuffle(f_mli)\n",
    "# f_mli = f_mli[::10]\n",
    "\n",
    "# # # debugging\n",
    "# # f_mli = f_mli[0:72*5]\n",
    "\n",
    "# random.shuffle(f_mli)\n",
    "# print(f'[TRAIN] Total # of input files: {len(f_mli)}')\n",
    "# print(f'[TRAIN] Total # of columns (nfiles * ncols): {len(f_mli)*384}')\n",
    "# tds = load_nc_dir_with_generator(f_mli)\n",
    "# tds = tds.unbatch()\n",
    "# tds = tds.shuffle(buffer_size=shuffle_buffer, reshuffle_each_iteration=True)\n",
    "# tds = tds.prefetch(buffer_size=4) # in realtion to the batch size\n",
    "\n",
    "# # for validation\n",
    "\n",
    "# # # First 5 days of each month for the following 2 years\n",
    "# # f_mli1 = glob.glob('/pscratch/sd/s/sungduk/hugging/E3SM-MMF_ne4/train/*/E3SM-MMF.mli.0007-0[23456789]-0[12345]-*.nc')\n",
    "# # f_mli2 = glob.glob('/pscratch/sd/s/sungduk/hugging/E3SM-MMF_ne4/train/*/E3SM-MMF.mli.0007-1[012]-0[12345]-*.nc')\n",
    "# # f_mli3 = glob.glob('/pscratch/sd/s/sungduk/hugging/E3SM-MMF_ne4/train/*/E3SM-MMF.mli.000[89]-*-0[12345]-*.nc')\n",
    "# # f_mli_val = [*f_mli1, *f_mli2, *f_mli3]\n",
    "\n",
    "# # every 10th sample\n",
    "# f_mli1 = glob.glob(root_train_path + '/*/E3SM-MMF.mli.0007-0[23456789]-0[12345]-*.nc')\n",
    "# f_mli2 = glob.glob(root_train_path + '/*/E3SM-MMF.mli.0007-1[012]-0[12345]-*.nc')\n",
    "# f_mli3 = glob.glob(root_train_path + '/*/E3SM-MMF.mli.000[89]-*-0[12345]-*.nc')\n",
    "# f_mli_val = sorted([*f_mli1, *f_mli2, *f_mli3])\n",
    "# f_mli_val = f_mli_val[::10]\n",
    "\n",
    "# # # debugging\n",
    "# # f_mli_val = f_mli_val[0:72*5]\n",
    "\n",
    "# random.shuffle(f_mli_val)\n",
    "# print(f'[VAL] Total # of input files: {len(f_mli_val)}')\n",
    "# print(f'[VAL] Total # of columns (nfiles * ncols): {len(f_mli_val)*384}')\n",
    "# tds_val = load_nc_dir_with_generator(f_mli_val)\n",
    "# tds_val = tds_val.shuffle(buffer_size=shuffle_buffer, reshuffle_each_iteration=True)\n",
    "# tds_val = tds_val.prefetch(buffer_size=4) # in realtion to the batch size\n",
    "\n",
    "# #list(tds)\n",
    "# # for count_batch in tds.repeat().batch(10).take(1):\n",
    "# #     print(count_batch[0].numpy())\n",
    "# #count_batch[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML training\n",
    "- While 4 GPUs are available on the node, using multi GPUs (with 'tf.distribute.MirroredStrategy()' strategy) does not speed up training process. It is possibly due to that the current Dataset pipeline is sequential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Emulator\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input (InputLayer)          [(None, 124)]                0         []                            \n",
      "                                                                                                  \n",
      " dense_10 (Dense)            (None, 512)                  64000     ['input[0][0]']               \n",
      "                                                                                                  \n",
      " dense_11 (Dense)            (None, 512)                  262656    ['dense_10[0][0]']            \n",
      "                                                                                                  \n",
      " dense_12 (Dense)            (None, 128)                  65664     ['dense_11[0][0]']            \n",
      "                                                                                                  \n",
      " dense_13 (Dense)            (None, 120)                  15480     ['dense_12[0][0]']            \n",
      "                                                                                                  \n",
      " dense_14 (Dense)            (None, 8)                    1032      ['dense_12[0][0]']            \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate  (None, 128)                  0         ['dense_13[0][0]',            \n",
      " )                                                                   'dense_14[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 408832 (1.56 MB)\n",
      "Trainable params: 408832 (1.56 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# strategy = tf.distribute.MirroredStrategy()\n",
    "# with strategy.scope():\n",
    "\n",
    "# model params\n",
    "input_length = 2*60 + 4\n",
    "output_length_lin  = 2*60\n",
    "output_length_relu = 8\n",
    "output_length = output_length_lin + output_length_relu\n",
    "n_nodes = 512\n",
    "\n",
    "# constrcut a model\n",
    "input_layer    = keras.layers.Input(shape=(input_length,), name='input')\n",
    "hidden_0       = keras.layers.Dense(n_nodes, activation='relu')(input_layer)\n",
    "hidden_1       = keras.layers.Dense(n_nodes, activation='relu')(hidden_0)\n",
    "output_pre     = keras.layers.Dense(output_length, activation='elu')(hidden_1)\n",
    "output_lin     = keras.layers.Dense(output_length_lin,activation='linear')(output_pre)\n",
    "output_relu    = keras.layers.Dense(output_length_relu,activation='relu')(output_pre)\n",
    "output_layer   = keras.layers.Concatenate()([output_lin, output_relu])\n",
    "\n",
    "model = keras.Model(input_layer, output_layer, name='Emulator')\n",
    "model.summary()\n",
    "\n",
    "# compile\n",
    "model.compile(optimizer=keras.optimizers.Adam(), #optimizer=keras.optimizers.Adam(learning_rate=clr),\n",
    "              loss='mse',\n",
    "              metrics=['mse','mae','accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks\n",
    "# a. tensorboard\n",
    "tboard_callback = keras.callbacks.TensorBoard(log_dir = './logs_tensorboard',\n",
    "                                              histogram_freq = 1,)\n",
    "\n",
    "# b. checkpoint\n",
    "filepath_checkpoint = 'saved_model/best_model_proto.h5'\n",
    "checkpoint_callback = keras.callbacks.ModelCheckpoint(filepath=filepath_checkpoint,\n",
    "                                                            save_weights_only=False,\n",
    "                                                            monitor='val_mse',\n",
    "                                                            mode='min',\n",
    "                                                            save_best_only=True)\n",
    "\n",
    "# c. csv logger\n",
    "filepath_csv = 'csv_logger.txt'\n",
    "csv_callback = keras.callbacks.CSVLogger(filepath_csv, separator=\",\", append=True)\n",
    "\n",
    "my_callbacks= [tboard_callback, checkpoint_callback, csv_callback]\n",
    "\n",
    "# !mkdir logs_tensorboard\n",
    "# !mkdir saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "800/800 [==============================] - 41s 50ms/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0328 - accuracy: 0.9544 - val_loss: 0.0048 - val_mse: 0.0048 - val_mae: 0.0293 - val_accuracy: 0.9656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alvarovh/.local/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n",
      "800/800 [==============================] - 17s 21ms/step - loss: 0.0048 - mse: 0.0048 - mae: 0.0280 - accuracy: 0.9624 - val_loss: 0.0045 - val_mse: 0.0045 - val_mae: 0.0267 - val_accuracy: 0.9660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alvarovh/.local/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\n",
      "800/800 [==============================] - 17s 21ms/step - loss: 0.0045 - mse: 0.0045 - mae: 0.0268 - accuracy: 0.9661 - val_loss: 0.0043 - val_mse: 0.0043 - val_mae: 0.0258 - val_accuracy: 0.9673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alvarovh/.local/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\n",
      "800/800 [==============================] - 17s 21ms/step - loss: 0.0045 - mse: 0.0045 - mae: 0.0261 - accuracy: 0.9665 - val_loss: 0.0042 - val_mse: 0.0042 - val_mae: 0.0249 - val_accuracy: 0.9660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alvarovh/.local/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\n",
      "800/800 [==============================] - 17s 21ms/step - loss: 0.0044 - mse: 0.0044 - mae: 0.0256 - accuracy: 0.9675 - val_loss: 0.0045 - val_mse: 0.0045 - val_mae: 0.0259 - val_accuracy: 0.9668\n",
      "Epoch: 6\n",
      "800/800 [==============================] - 17s 21ms/step - loss: 0.0043 - mse: 0.0043 - mae: 0.0252 - accuracy: 0.9677 - val_loss: 0.0043 - val_mse: 0.0043 - val_mae: 0.0247 - val_accuracy: 0.9669\n",
      "Epoch: 7\n",
      "800/800 [==============================] - 17s 21ms/step - loss: 0.0043 - mse: 0.0043 - mae: 0.0248 - accuracy: 0.9686 - val_loss: 0.0042 - val_mse: 0.0042 - val_mae: 0.0243 - val_accuracy: 0.9703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alvarovh/.local/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8\n",
      "800/800 [==============================] - 17s 21ms/step - loss: 0.0042 - mse: 0.0042 - mae: 0.0246 - accuracy: 0.9694 - val_loss: 0.0042 - val_mse: 0.0042 - val_mae: 0.0243 - val_accuracy: 0.9669\n",
      "Epoch: 9\n",
      "800/800 [==============================] - 17s 21ms/step - loss: 0.0042 - mse: 0.0042 - mae: 0.0243 - accuracy: 0.9696 - val_loss: 0.0043 - val_mse: 0.0043 - val_mae: 0.0244 - val_accuracy: 0.9672\n",
      "Epoch: 10\n",
      "800/800 [==============================] - 17s 21ms/step - loss: 0.0041 - mse: 0.0041 - mae: 0.0241 - accuracy: 0.9704 - val_loss: 0.0043 - val_mse: 0.0043 - val_mae: 0.0246 - val_accuracy: 0.9690\n",
      "Epoch: 11\n",
      "    797/Unknown - 15s 19ms/step - loss: 0.0041 - mse: 0.0041 - mae: 0.0239 - accuracy: 0.9715"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alvarovh/.local/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 17s 22ms/step - loss: 0.0041 - mse: 0.0041 - mae: 0.0239 - accuracy: 0.9715 - val_loss: 0.0040 - val_mse: 0.0040 - val_mae: 0.0238 - val_accuracy: 0.9704\n",
      "Epoch: 12\n",
      "800/800 [==============================] - 17s 21ms/step - loss: 0.0041 - mse: 0.0041 - mae: 0.0239 - accuracy: 0.9713 - val_loss: 0.0040 - val_mse: 0.0040 - val_mae: 0.0234 - val_accuracy: 0.9730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alvarovh/.local/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13\n",
      "800/800 [==============================] - 17s 21ms/step - loss: 0.0040 - mse: 0.0040 - mae: 0.0237 - accuracy: 0.9709 - val_loss: 0.0040 - val_mse: 0.0040 - val_mae: 0.0233 - val_accuracy: 0.9661\n",
      "Epoch: 14\n",
      "800/800 [==============================] - 17s 21ms/step - loss: 0.0040 - mse: 0.0040 - mae: 0.0235 - accuracy: 0.9716 - val_loss: 0.0039 - val_mse: 0.0039 - val_mae: 0.0230 - val_accuracy: 0.9711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alvarovh/.local/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15\n",
      "    797/Unknown - 15s 19ms/step - loss: 0.0040 - mse: 0.0040 - mae: 0.0234 - accuracy: 0.9719"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alvarovh/.local/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 17s 21ms/step - loss: 0.0040 - mse: 0.0040 - mae: 0.0234 - accuracy: 0.9718 - val_loss: 0.0039 - val_mse: 0.0039 - val_mae: 0.0232 - val_accuracy: 0.9719\n",
      "Epoch: 16\n",
      "800/800 [==============================] - 17s 21ms/step - loss: 0.0040 - mse: 0.0040 - mae: 0.0233 - accuracy: 0.9729 - val_loss: 0.0039 - val_mse: 0.0039 - val_mae: 0.0232 - val_accuracy: 0.9711\n",
      "Epoch: 17\n",
      "800/800 [==============================] - 17s 21ms/step - loss: 0.0039 - mse: 0.0039 - mae: 0.0233 - accuracy: 0.9721 - val_loss: 0.0040 - val_mse: 0.0040 - val_mae: 0.0230 - val_accuracy: 0.9724\n",
      "Epoch: 18\n",
      "800/800 [==============================] - 17s 21ms/step - loss: 0.0039 - mse: 0.0039 - mae: 0.0232 - accuracy: 0.9726 - val_loss: 0.0038 - val_mse: 0.0038 - val_mae: 0.0229 - val_accuracy: 0.9723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alvarovh/.local/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19\n",
      "800/800 [==============================] - 17s 21ms/step - loss: 0.0039 - mse: 0.0039 - mae: 0.0231 - accuracy: 0.9726 - val_loss: 0.0039 - val_mse: 0.0039 - val_mae: 0.0234 - val_accuracy: 0.9677\n",
      "Epoch: 20\n",
      "800/800 [==============================] - 17s 21ms/step - loss: 0.0039 - mse: 0.0039 - mae: 0.0230 - accuracy: 0.9729 - val_loss: 0.0039 - val_mse: 0.0039 - val_mae: 0.0232 - val_accuracy: 0.9730\n",
      "Epoch: 21\n",
      "    797/Unknown - 15s 19ms/step - loss: 0.0039 - mse: 0.0039 - mae: 0.0230 - accuracy: 0.9730"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alvarovh/.local/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 19s 23ms/step - loss: 0.0039 - mse: 0.0039 - mae: 0.0230 - accuracy: 0.9731 - val_loss: 0.0038 - val_mse: 0.0038 - val_mae: 0.0227 - val_accuracy: 0.9738\n",
      "Epoch: 22\n",
      "800/800 [==============================] - 17s 21ms/step - loss: 0.0039 - mse: 0.0039 - mae: 0.0229 - accuracy: 0.9735 - val_loss: 0.0038 - val_mse: 0.0038 - val_mae: 0.0227 - val_accuracy: 0.9753\n",
      "Epoch: 23\n",
      "800/800 [==============================] - 17s 21ms/step - loss: 0.0038 - mse: 0.0038 - mae: 0.0228 - accuracy: 0.9741 - val_loss: 0.0038 - val_mse: 0.0038 - val_mae: 0.0229 - val_accuracy: 0.9733\n",
      "Epoch: 24\n",
      "800/800 [==============================] - 17s 21ms/step - loss: 0.0038 - mse: 0.0038 - mae: 0.0227 - accuracy: 0.9738 - val_loss: 0.0038 - val_mse: 0.0038 - val_mae: 0.0226 - val_accuracy: 0.9746\n",
      "Epoch: 25\n",
      "800/800 [==============================] - 17s 21ms/step - loss: 0.0038 - mse: 0.0038 - mae: 0.0227 - accuracy: 0.9736 - val_loss: 0.0038 - val_mse: 0.0038 - val_mae: 0.0228 - val_accuracy: 0.9708\n",
      "Epoch: 26\n",
      "800/800 [==============================] - 17s 21ms/step - loss: 0.0038 - mse: 0.0038 - mae: 0.0227 - accuracy: 0.9751 - val_loss: 0.0039 - val_mse: 0.0039 - val_mae: 0.0229 - val_accuracy: 0.9736\n",
      "Epoch: 27\n",
      "800/800 [==============================] - 17s 21ms/step - loss: 0.0038 - mse: 0.0038 - mae: 0.0226 - accuracy: 0.9753 - val_loss: 0.0037 - val_mse: 0.0037 - val_mae: 0.0224 - val_accuracy: 0.9743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alvarovh/.local/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28\n",
      "800/800 [==============================] - 17s 21ms/step - loss: 0.0038 - mse: 0.0038 - mae: 0.0226 - accuracy: 0.9741 - val_loss: 0.0037 - val_mse: 0.0037 - val_mae: 0.0227 - val_accuracy: 0.9764\n",
      "Epoch: 29\n",
      "800/800 [==============================] - 17s 21ms/step - loss: 0.0037 - mse: 0.0037 - mae: 0.0225 - accuracy: 0.9752 - val_loss: 0.0037 - val_mse: 0.0037 - val_mae: 0.0224 - val_accuracy: 0.9753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alvarovh/.local/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30\n",
      "800/800 [==============================] - 17s 21ms/step - loss: 0.0037 - mse: 0.0037 - mae: 0.0225 - accuracy: 0.9752 - val_loss: 0.0037 - val_mse: 0.0037 - val_mae: 0.0223 - val_accuracy: 0.9755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alvarovh/.local/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Manually shuffling the order of input files.\n",
    "# \"tds = tds.shuffle(buffer_size=<global>, reshuffle_each_iteration=True)\" is possible,\n",
    "# however, it is slow.\n",
    "# So employing global shuffle (by file names) + local shuffle (using .shuffle).\n",
    "\n",
    "N_EPOCHS = 30\n",
    "shuffle_buffer = 12*384 #ncol=384\n",
    "batch_size= 96 # 384/4\n",
    "\n",
    "n=0\n",
    "while n < N_EPOCHS:\n",
    "    random.shuffle(f_mli)\n",
    "    tds = load_nc_dir_with_generator(f_mli) # global shuffle by file names\n",
    "    tds = tds.unbatch()\n",
    " # local shuffle by elements    tds = tds.shuffle(buffer_size=shuffle_buffer, reshuffle_each_iteration=False)\n",
    "    tds = tds.batch(batch_size)\n",
    "    tds = tds.prefetch(buffer_size=int(shuffle_buffer/384)) # in realtion to the batch size\n",
    "\n",
    "    random.shuffle(f_mli_val)\n",
    "    tds_val = load_nc_dir_with_generator(f_mli_val)\n",
    "    tds_val = tds_val.unbatch()\n",
    "    tds_val = tds_val.shuffle(buffer_size=shuffle_buffer, reshuffle_each_iteration=False)\n",
    "    tds_val = tds_val.batch(batch_size)\n",
    "    tds_val = tds_val.prefetch(buffer_size=int(shuffle_buffer/384))\n",
    "    \n",
    "    print(f'Epoch: {n+1}')\n",
    "    model.fit(tds, \n",
    "              validation_data=tds_val,\n",
    "              callbacks=my_callbacks)\n",
    "    \n",
    "    n+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f6473a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/alvarovh'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
